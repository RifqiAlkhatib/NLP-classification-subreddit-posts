{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Creating Classification Model](#Creating-Classification-Model)\n",
    "- [Fitting Model to Train Data](#Fitting-Model-to-Train-Data)\n",
    "- [Results](#Results)\n",
    "- [Multinomial Naive Bayes vs Support Vector Classification](#Multinomial-Naive-Bayes-vs-Support-Vector-Classification)\n",
    "- [Model Limitations](#Model-Limitations)\n",
    "- [Conclusions & Recommendations](#Conclusions-&-Recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import regex as re\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier, \n",
    "                              AdaBoostClassifier, GradientBoostingClassifier)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Classification Model\n",
    "\n",
    "In this section, we will be trying to find and train the best model to resolve our binary classification problem - predicting the subreddit that a post belongs to. The model will consist of a vectorizer and classifier that will be selected & tuned based on several evaluation metrics, such as accuracy & ROC-AUC.\n",
    "\n",
    "The methodology will be as follows:\n",
    "\n",
    "1. Generate baseline model\n",
    "2. Train test split\n",
    "3. Instantiate multiple vectorizers & classifiers with their respective parameter grids\n",
    "4. Create & fit model to training data\n",
    "    - Vectorizer to transform data\n",
    "    - Classifier to generate predictions\n",
    "    - GridSearchCV to find best hyperparameters for each\n",
    "5. Evaluate different models based on scoring metrics\n",
    "6. Select top 2 models for further study    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/combined_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting target column to numeric value\n",
    "data['is_askscience'] = data['subreddit'].map(lambda x: 1 if x == 'askscience' else 0)\n",
    "data.drop(columns='subreddit', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>is_askscience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>read entir post care format applic appropri po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tl dr week british scienc week answer scienc t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>graviti electromagnet quit similar forc base m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mrna pfizer moderna ad26 j j goal vaccin make ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virus lead cancer later life interact happen v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  is_askscience\n",
       "0  read entir post care format applic appropri po...              1\n",
       "1  tl dr week british scienc week answer scienc t...              1\n",
       "2  graviti electromagnet quit similar forc base m...              1\n",
       "3  mrna pfizer moderna ad26 j j goal vaccin make ...              1\n",
       "4  virus lead cancer later life interact happen v...              1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "We will be using the actual distribution of the target variable as our baseline, i.e. the accuracy score that our model ***must*** beat. This baseline can be interpreted as a 54% accuracy score when predicting all posts to be from `r/askscience`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.540437\n",
       "0    0.459563\n",
       "Name: is_askscience, dtype: float64"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline model\n",
    "data['is_askscience'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['clean_text'],\n",
    "    data['is_askscience'],\n",
    "    stratify=data['is_askscience'],\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizers\n",
    "\n",
    "We will be using 3 different vectorizers to transform our text data into a form that is readable by a machine learning model.\n",
    "\n",
    "#### Count Vectorizer\n",
    "\n",
    "Count Vectorizer is used to transform the corpus into a vector of token counts. The raw text is converted into a numerical vector representation of words (and larger n-grams, if specified), which then makes it possible for the data to be used as features for the machine learning model. The transformed data is stored in the sparse matrix format, which greatly reduces memory usage and speeds up computation. Basically, the raw text is tokenised, then the occurences of each token are counted up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "cvec = CountVectorizer()\n",
    "cvec_params = {\n",
    "    'vec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'vec__max_features': [None, 5000, 2500, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf Vectorizer\n",
    "\n",
    "Tf-idf, or term-frequency times inverse document-frequency, is similar to CountVectorizer except that the frequency of each token is multiplied by the idf component. This acts as a pseudo-regularisation of tokens that appear very often in the corpus and may not be very meaningful when input into the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "tvec = TfidfVectorizer()\n",
    "tvec_params = {\n",
    "    'vec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'vec__max_features': [None, 5000, 4000, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing Vectorizer\n",
    "\n",
    "Hashing Vectorizer serves the same purpose as Count Vectorizer (to transform raw text data into a matrix of token occurrences), except for one key difference - Hashing Vectorizer does not store the resulting vocabulary. Each token is instead mapped to a column position in a matrix via hashing. This method is more efficient for large datasets as the vocabulary is not required to be stored in memory. However, because of this, we are unable to access the actual tokens after the transformation, making the model unable to be interpreted properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashing Vectorizer\n",
    "hvec = HashingVectorizer()\n",
    "hvec_params = {\n",
    "    'vec__n_features': [2**17, 2**18, 2**19],\n",
    "    'vec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'vec__norm': ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers\n",
    "\n",
    "We will be exploring the use of 10 different classifiers.\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "Logistic Regression is a linear classification model that gives us the probabilities of being in each class by applying the logit link function to a linear regression model. Each feature is assigned a coefficient which represents the feature's weight on predicting the log-odds of success of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression(\n",
    "    solver='liblinear',\n",
    "    max_iter=1000, \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "logreg_params = {\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__C': [0.1, 0.5, 0.75, 1, 1.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors\n",
    "\n",
    "K Nearest Neighbors (KNN) is a non-parametric model that memorises the 'locations' of each observation in relation to the each feature when fit to the training dataset. The distance between each known observation and any new unclassified point is then calculated, and the new point is assigned a class based on its proximity to other points of the each class. `k` is the hyperparameter that represents the number of closest neighbors that 'vote' on the class assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "knn_params = {\n",
    "    'clf__n_neighbors': [3, 5, 7, 9],\n",
    "    'clf__weights': ['uniform', 'distance'],\n",
    "    'clf__p': [1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes\n",
    "\n",
    "Multinomial Naive Bayes (NB) is a classification model that relies on Bayes' theorem and the assumption that each feature in the dataset is independent of one another to classify points. The probability that an observation belongs to a specific class is calculated based on Bayes' theorem:\n",
    "\n",
    "$$\n",
    "Pr(A | B) = \\frac{Pr(B | A) Pr(A)}{Pr(B)}\n",
    "$$\n",
    "\n",
    "Where `P(A)` represents the probability of the observation belonging to a specific class while `P(B)` represents the probability of observing the words in the observation. Multinomial NB is used for this classification model as the features (token occurences) are discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb_params = {\n",
    "    'clf__alpha': np.logspace(-4, 2, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "\n",
    "Decision Trees (DTs) are classification trees that begin with a root node, through which all observations are passed. The dataset is then partitioned at every internal split based on the drop in Gini impurity. DTs are fit using a greedy algorithm, meaning that they calculate the Gini impurity for every possible split at every node, thus making it a locally optimised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dt_params = {\n",
    "    'clf__max_depth': [5, 7, 9],\n",
    "    'clf__ccp_alpha': [0, 0.1, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging\n",
    "\n",
    "Bagging, or Bootstrap Aggregating, is an ensemble method that bootstraps (random resampling with replacement) multiple samples from the original data, then builds a decision tree on each bootstrapped sample. The predictions are then made by passing each test observation through all the decision trees and then aggregating a prediction for that observation. This helps to reduce the overfitting problem with single DTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "bag = BaggingClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "bag_params = {\n",
    "    'clf__n_estimators': [5, 10, 15]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "Random Forest goes a step further than Bagging and tries to reduce the correlation of the individual decision trees with each other by selecting a random subset of features at each split. This greatly reduces the variance of the model, at the expense of a small increase in bias, making in generally superior to full-sized bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "rf_params = {\n",
    "    'clf__n_estimators': [100, 150, 200],\n",
    "    'clf__max_depth': [4, 5, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Trees\n",
    "\n",
    "Extra Trees, or Extremely Randomized Trees, goes even further by adding another layer of randomization in each individual decision tree. Instead of deciding the split based on the drop in Gini impurity, a random value is selected for the split (selected from the features' empirical range). This results in an additional reduction in model variance and increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees\n",
    "et = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "et_params = {\n",
    "    'clf__n_estimators': [50, 75, 100],\n",
    "    'clf__max_depth': [5, 6, 7]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ada Boost\n",
    "\n",
    "Ada Boost is a form of boosting, which is another ensemble method that takes a weak base learner (e.g. DT with only 1 or 2 splits) and then tries to make it a strong learner by retraining it on the misclassified samples. This is an iterative procedure that assigns weights to observations at each iteration to indicate their 'importance'. Weights are also assigned to each base model at each iteration that determines their 'say' in the final vote that would determine the model's prediction.\n",
    "\n",
    "As boosting is an iterative procedure that cannot be parallelised, we will not go too deep into tuning the hyperparameters as it would take too long a time to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ada Boost\n",
    "ada = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(), \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ada_params = {\n",
    "    'clf__base_estimator__max_depth': [1, 2],\n",
    "    'clf__learning_rate': [0.8, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost\n",
    "\n",
    "Gradient Boost is another form of boosting that instead fits each subsequent model to the residuals of the last model. As with Ada Boost, we will only be exploring some hyperparameters for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boost\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "gb_params = {\n",
    "    'clf__learning_rate': [0.1, 0.2],\n",
    "    'clf__max_depth': [2, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine\n",
    "\n",
    "Support Vector Machines (SVMs) are statistical models used for classification, and works by finding hyperplanes that separate linearly separable classes with maximum margin. SVM uses numerical methods to solve for decision boundaries that minimise the hinge loss. In other words, it finds the hyperplane boundary between classes that results in the least penalisation (due to misclassified or ambiguous points). SVMs are able to work with data that is not linearly separable by using different kernels that apply functions to the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svc = svm.SVC(random_state=42)\n",
    "\n",
    "svc_params = {\n",
    "    'clf__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'clf__C': [1, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Model to Train Data\n",
    "\n",
    "We will be using a pipeline and GridSearchCV with all possible combinations of vectorizers and classifiers to create a model that will be fit to the training dataset. In this project's context, we will be optimising the hyperparameters based on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate result list\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to model different combinations of vectorizers & classifiers, and store results\n",
    "def model(vec, clf, vec_params, clf_params):\n",
    "    \n",
    "    # Instantiate pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('vec', vec),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    # Scale for knn, with_mean = False to deal with sparse matrices\n",
    "    if clf == knn:\n",
    "        pipe = Pipeline([\n",
    "            ('vec', vec),\n",
    "            ('ss', StandardScaler(with_mean=False)),\n",
    "            ('clf', clf)\n",
    "        ])\n",
    "        \n",
    "    # Default vectorizer params for boosting classifiers for ease of computation\n",
    "    if clf == ada or clf == gb:\n",
    "        vec_params = {}\n",
    "    \n",
    "    # Gridsearch for best estimator\n",
    "    grid = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grid={**vec_params, **clf_params},\n",
    "        scoring='accuracy',\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(f'{vec}: {clf}')\n",
    "    print('Best Parameters:')\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    # Scoring metrics\n",
    "    scores = {'Vectorizer': vec, 'Classifier': clf}\n",
    "    y_preds = grid.predict(X_test)\n",
    "    scores['Train Accuracy Score'] = grid.score(X_train, y_train)\n",
    "    scores['Test Accuracy Score'] = metrics.accuracy_score(y_test, y_preds)\n",
    "    scores['F1'] = metrics.f1_score(y_test, y_preds)\n",
    "    scores['Precision'] = metrics.precision_score(y_test, y_preds)\n",
    "    scores['Recall'] = metrics.recall_score(y_test, y_preds)\n",
    "    scores['ROC-AUC'] = metrics.roc_auc_score(y_test, y_preds)\n",
    "    \n",
    "    # Storing results\n",
    "    \n",
    "    results.append(scores)\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = [\n",
    "    (cvec, cvec_params),\n",
    "    (tvec, tvec_params),\n",
    "    (hvec, hvec_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    (logreg, logreg_params),\n",
    "    (knn, knn_params),\n",
    "    (mnb, mnb_params),\n",
    "    (dt, dt_params),\n",
    "    (bag, bag_params),\n",
    "    (rf, rf_params),\n",
    "    (et, et_params),\n",
    "    (ada, ada_params),\n",
    "    (gb, gb_params),\n",
    "    (svc, svc_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   22.5s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   45.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')\n",
      "Best Parameters:\n",
      "{'clf__C': 0.5, 'clf__penalty': 'l2', 'vec__max_features': None, 'vec__ngram_range': (1, 1)}\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_jobs=-1)\n",
      "Best Parameters:\n",
      "{'clf__n_neighbors': 3, 'clf__p': 2, 'clf__weights': 'distance', 'vec__max_features': 1000, 'vec__ngram_range': (1, 2)}\n",
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 381 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   48.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "Best Parameters:\n",
      "{'clf__alpha': 1.0, 'vec__max_features': 5000, 'vec__ngram_range': (1, 1)}\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:   48.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__ccp_alpha': 0, 'clf__max_depth': 9, 'vec__max_features': 2500, 'vec__ngram_range': (1, 2)}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(n_jobs=-1, random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__n_estimators': 5, 'vec__max_features': 5000, 'vec__ngram_range': (1, 2)}\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   40.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_jobs=-1, random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 6, 'clf__n_estimators': 150, 'vec__max_features': 1000, 'vec__ngram_range': (1, 1)}\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   58.7s\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 7, 'clf__n_estimators': 75, 'vec__max_features': 1000, 'vec__ngram_range': (1, 2)}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:    6.4s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    7.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__base_estimator__max_depth': 2, 'clf__learning_rate': 1}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:    7.5s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    8.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__learning_rate': 0.2, 'clf__max_depth': 3}\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__C': 1, 'clf__kernel': 'linear', 'vec__max_features': None, 'vec__ngram_range': (1, 1)}\n",
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   54.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')\n",
      "Best Parameters:\n",
      "{'clf__C': 1, 'clf__penalty': 'l2', 'vec__max_features': 4000, 'vec__ngram_range': (1, 1)}\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_jobs=-1)\n",
      "Best Parameters:\n",
      "{'clf__n_neighbors': 3, 'clf__p': 2, 'clf__weights': 'distance', 'vec__max_features': None, 'vec__ngram_range': (1, 1)}\n",
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 381 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   50.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "Best Parameters:\n",
      "{'clf__alpha': 0.21544346900318823, 'vec__max_features': None, 'vec__ngram_range': (1, 2)}\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:   53.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__ccp_alpha': 0, 'clf__max_depth': 9, 'vec__max_features': None, 'vec__ngram_range': (1, 2)}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(n_jobs=-1, random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__n_estimators': 15, 'vec__max_features': None, 'vec__ngram_range': (1, 1)}\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   35.8s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_jobs=-1, random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 6, 'clf__n_estimators': 100, 'vec__max_features': 1000, 'vec__ngram_range': (1, 1)}\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   53.4s\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 7, 'clf__n_estimators': 100, 'vec__max_features': 1000, 'vec__ngram_range': (1, 1)}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:    2.9s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__base_estimator__max_depth': 1, 'clf__learning_rate': 0.8}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:    6.8s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    7.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__learning_rate': 0.2, 'clf__max_depth': 3}\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   27.3s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__C': 1, 'clf__kernel': 'linear', 'vec__max_features': None, 'vec__ngram_range': (1, 2)}\n",
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 186 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=-1)]: Done 672 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')\n",
      "Best Parameters:\n",
      "{'clf__C': 1.5, 'clf__penalty': 'l2', 'vec__n_features': 131072, 'vec__ngram_range': (1, 1), 'vec__norm': 'l2'}\n",
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_jobs=-1)\n",
      "Best Parameters:\n",
      "{'clf__n_neighbors': 3, 'clf__p': 2, 'clf__weights': 'distance', 'vec__n_features': 131072, 'vec__ngram_range': (1, 1), 'vec__norm': 'l1'}\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   51.1s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__ccp_alpha': 0, 'clf__max_depth': 9, 'vec__n_features': 262144, 'vec__ngram_range': (1, 2), 'vec__norm': 'l1'}\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed: 17.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(n_jobs=-1, random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__n_estimators': 15, 'vec__n_features': 262144, 'vec__ngram_range': (1, 1), 'vec__norm': 'l2'}\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_jobs=-1, random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 6, 'clf__n_estimators': 200, 'vec__n_features': 131072, 'vec__ngram_range': (1, 2), 'vec__norm': 'l2'}\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   45.1s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 7, 'clf__n_estimators': 50, 'vec__n_features': 131072, 'vec__ngram_range': (1, 2), 'vec__norm': 'l2'}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:   56.5s remaining:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__base_estimator__max_depth': 2, 'clf__learning_rate': 0.8}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:  2.3min remaining:   35.1s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__learning_rate': 0.2, 'clf__max_depth': 3}\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   45.6s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(random_state=42)\n",
      "Best Parameters:\n",
      "{'clf__C': 1, 'clf__kernel': 'sigmoid', 'vec__n_features': 262144, 'vec__ngram_range': (1, 2), 'vec__norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "for (vec, vec_params) in vectorizers:\n",
    "    for (clf, clf_params) in classifiers:\n",
    "        # MNB Classifier cannot take in negative X values from hvec\n",
    "        if not(vec == hvec and clf == mnb):\n",
    "            model(vec, clf, vec_params, clf_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking different models by test accuracy score\n",
    "model_results = pd.DataFrame(results)\\\n",
    ".sort_values(by='Test Accuracy Score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Train Accuracy Score</th>\n",
       "      <th>Test Accuracy Score</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958106</td>\n",
       "      <td>0.960549</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.942761</td>\n",
       "      <td>0.959476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.948998</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.985560</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.951659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(random_state=42)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943534</td>\n",
       "      <td>0.948074</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>0.952862</td>\n",
       "      <td>0.942701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HashingVectorizer()</td>\n",
       "      <td>SVC(random_state=42)</td>\n",
       "      <td>0.984387</td>\n",
       "      <td>0.927140</td>\n",
       "      <td>0.932886</td>\n",
       "      <td>0.929766</td>\n",
       "      <td>0.936027</td>\n",
       "      <td>0.926347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>LogisticRegression(max_iter=1000, random_state...</td>\n",
       "      <td>0.985168</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.927066</td>\n",
       "      <td>0.893750</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.914021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer()</td>\n",
       "      <td>LogisticRegression(max_iter=1000, random_state...</td>\n",
       "      <td>0.978142</td>\n",
       "      <td>0.914390</td>\n",
       "      <td>0.924559</td>\n",
       "      <td>0.883436</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.909452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>LogisticRegression(max_iter=1000, random_state...</td>\n",
       "      <td>0.999219</td>\n",
       "      <td>0.907104</td>\n",
       "      <td>0.918138</td>\n",
       "      <td>0.877301</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.902116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(random_state=42)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897996</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.882540</td>\n",
       "      <td>0.936027</td>\n",
       "      <td>0.894601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer()</td>\n",
       "      <td>GradientBoostingClassifier(random_state=42)</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.863388</td>\n",
       "      <td>0.882995</td>\n",
       "      <td>0.822674</td>\n",
       "      <td>0.952862</td>\n",
       "      <td>0.855399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>GradientBoostingClassifier(random_state=42)</td>\n",
       "      <td>0.986729</td>\n",
       "      <td>0.854281</td>\n",
       "      <td>0.875389</td>\n",
       "      <td>0.814493</td>\n",
       "      <td>0.946128</td>\n",
       "      <td>0.846080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>GradientBoostingClassifier(random_state=42)</td>\n",
       "      <td>0.964091</td>\n",
       "      <td>0.850638</td>\n",
       "      <td>0.872671</td>\n",
       "      <td>0.809798</td>\n",
       "      <td>0.946128</td>\n",
       "      <td>0.842112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>AdaBoostClassifier(base_estimator=DecisionTree...</td>\n",
       "      <td>0.977361</td>\n",
       "      <td>0.850638</td>\n",
       "      <td>0.866883</td>\n",
       "      <td>0.836991</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.846320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>BaggingClassifier(n_jobs=-1, random_state=42)</td>\n",
       "      <td>0.993755</td>\n",
       "      <td>0.850638</td>\n",
       "      <td>0.866883</td>\n",
       "      <td>0.836991</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.846320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HashingVectorizer()</td>\n",
       "      <td>BaggingClassifier(n_jobs=-1, random_state=42)</td>\n",
       "      <td>0.994536</td>\n",
       "      <td>0.846995</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.833856</td>\n",
       "      <td>0.895623</td>\n",
       "      <td>0.842653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>BaggingClassifier(n_jobs=-1, random_state=42)</td>\n",
       "      <td>0.971897</td>\n",
       "      <td>0.841530</td>\n",
       "      <td>0.857610</td>\n",
       "      <td>0.834395</td>\n",
       "      <td>0.882155</td>\n",
       "      <td>0.837903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>AdaBoostClassifier(base_estimator=DecisionTree...</td>\n",
       "      <td>0.915691</td>\n",
       "      <td>0.826958</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.806061</td>\n",
       "      <td>0.895623</td>\n",
       "      <td>0.820827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HashingVectorizer()</td>\n",
       "      <td>AdaBoostClassifier(base_estimator=DecisionTree...</td>\n",
       "      <td>0.987510</td>\n",
       "      <td>0.825137</td>\n",
       "      <td>0.841060</td>\n",
       "      <td>0.827362</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>0.822451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>DecisionTreeClassifier(random_state=42)</td>\n",
       "      <td>0.850117</td>\n",
       "      <td>0.799636</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>0.759003</td>\n",
       "      <td>0.922559</td>\n",
       "      <td>0.788660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>RandomForestClassifier(n_jobs=-1, random_state...</td>\n",
       "      <td>0.846214</td>\n",
       "      <td>0.797814</td>\n",
       "      <td>0.839363</td>\n",
       "      <td>0.736041</td>\n",
       "      <td>0.976431</td>\n",
       "      <td>0.781866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>RandomForestClassifier(n_jobs=-1, random_state...</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.795993</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.737789</td>\n",
       "      <td>0.966330</td>\n",
       "      <td>0.780784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>ExtraTreesClassifier(n_jobs=-1, random_state=42)</td>\n",
       "      <td>0.839969</td>\n",
       "      <td>0.790528</td>\n",
       "      <td>0.835007</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.773629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>HashingVectorizer()</td>\n",
       "      <td>DecisionTreeClassifier(random_state=42)</td>\n",
       "      <td>0.846214</td>\n",
       "      <td>0.788707</td>\n",
       "      <td>0.825826</td>\n",
       "      <td>0.745257</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.776455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>DecisionTreeClassifier(random_state=42)</td>\n",
       "      <td>0.853240</td>\n",
       "      <td>0.788707</td>\n",
       "      <td>0.823708</td>\n",
       "      <td>0.750693</td>\n",
       "      <td>0.912458</td>\n",
       "      <td>0.777658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>ExtraTreesClassifier(n_jobs=-1, random_state=42)</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.726776</td>\n",
       "      <td>0.795640</td>\n",
       "      <td>0.668192</td>\n",
       "      <td>0.983165</td>\n",
       "      <td>0.703884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>KNeighborsClassifier(n_jobs=-1)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690346</td>\n",
       "      <td>0.769022</td>\n",
       "      <td>0.644647</td>\n",
       "      <td>0.952862</td>\n",
       "      <td>0.666907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>KNeighborsClassifier(n_jobs=-1)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.681239</td>\n",
       "      <td>0.753868</td>\n",
       "      <td>0.647343</td>\n",
       "      <td>0.902357</td>\n",
       "      <td>0.661496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>HashingVectorizer()</td>\n",
       "      <td>RandomForestClassifier(n_jobs=-1, random_state...</td>\n",
       "      <td>0.701015</td>\n",
       "      <td>0.661202</td>\n",
       "      <td>0.760925</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.631253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>HashingVectorizer()</td>\n",
       "      <td>ExtraTreesClassifier(n_jobs=-1, random_state=42)</td>\n",
       "      <td>0.663544</td>\n",
       "      <td>0.597450</td>\n",
       "      <td>0.728834</td>\n",
       "      <td>0.573359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.561508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>HashingVectorizer()</td>\n",
       "      <td>KNeighborsClassifier(n_jobs=-1)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>0.708181</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.976431</td>\n",
       "      <td>0.527898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Vectorizer                                         Classifier  \\\n",
       "0     TfidfVectorizer()                                    MultinomialNB()   \n",
       "1     CountVectorizer()                                    MultinomialNB()   \n",
       "2     TfidfVectorizer()                               SVC(random_state=42)   \n",
       "3   HashingVectorizer()                               SVC(random_state=42)   \n",
       "4     TfidfVectorizer()  LogisticRegression(max_iter=1000, random_state...   \n",
       "5   HashingVectorizer()  LogisticRegression(max_iter=1000, random_state...   \n",
       "6     CountVectorizer()  LogisticRegression(max_iter=1000, random_state...   \n",
       "7     CountVectorizer()                               SVC(random_state=42)   \n",
       "8   HashingVectorizer()        GradientBoostingClassifier(random_state=42)   \n",
       "9     TfidfVectorizer()        GradientBoostingClassifier(random_state=42)   \n",
       "10    CountVectorizer()        GradientBoostingClassifier(random_state=42)   \n",
       "11    CountVectorizer()  AdaBoostClassifier(base_estimator=DecisionTree...   \n",
       "12    TfidfVectorizer()      BaggingClassifier(n_jobs=-1, random_state=42)   \n",
       "13  HashingVectorizer()      BaggingClassifier(n_jobs=-1, random_state=42)   \n",
       "14    CountVectorizer()      BaggingClassifier(n_jobs=-1, random_state=42)   \n",
       "15    TfidfVectorizer()  AdaBoostClassifier(base_estimator=DecisionTree...   \n",
       "16  HashingVectorizer()  AdaBoostClassifier(base_estimator=DecisionTree...   \n",
       "17    TfidfVectorizer()            DecisionTreeClassifier(random_state=42)   \n",
       "18    CountVectorizer()  RandomForestClassifier(n_jobs=-1, random_state...   \n",
       "19    TfidfVectorizer()  RandomForestClassifier(n_jobs=-1, random_state...   \n",
       "20    TfidfVectorizer()   ExtraTreesClassifier(n_jobs=-1, random_state=42)   \n",
       "21  HashingVectorizer()            DecisionTreeClassifier(random_state=42)   \n",
       "22    CountVectorizer()            DecisionTreeClassifier(random_state=42)   \n",
       "23    CountVectorizer()   ExtraTreesClassifier(n_jobs=-1, random_state=42)   \n",
       "24    CountVectorizer()                    KNeighborsClassifier(n_jobs=-1)   \n",
       "25    TfidfVectorizer()                    KNeighborsClassifier(n_jobs=-1)   \n",
       "26  HashingVectorizer()  RandomForestClassifier(n_jobs=-1, random_state...   \n",
       "27  HashingVectorizer()   ExtraTreesClassifier(n_jobs=-1, random_state=42)   \n",
       "28  HashingVectorizer()                    KNeighborsClassifier(n_jobs=-1)   \n",
       "\n",
       "    Train Accuracy Score  Test Accuracy Score        F1  Precision    Recall  \\\n",
       "0               1.000000             0.958106  0.960549   0.979021  0.942761   \n",
       "1               0.983607             0.948998  0.951220   0.985560  0.919192   \n",
       "2               1.000000             0.943534  0.948074   0.943333  0.952862   \n",
       "3               0.984387             0.927140  0.932886   0.929766  0.936027   \n",
       "4               0.985168             0.918033  0.927066   0.893750  0.962963   \n",
       "5               0.978142             0.914390  0.924559   0.883436  0.969697   \n",
       "6               0.999219             0.907104  0.918138   0.877301  0.962963   \n",
       "7               1.000000             0.897996  0.908497   0.882540  0.936027   \n",
       "8               0.983607             0.863388  0.882995   0.822674  0.952862   \n",
       "9               0.986729             0.854281  0.875389   0.814493  0.946128   \n",
       "10              0.964091             0.850638  0.872671   0.809798  0.946128   \n",
       "11              0.977361             0.850638  0.866883   0.836991  0.898990   \n",
       "12              0.993755             0.850638  0.866883   0.836991  0.898990   \n",
       "13              0.994536             0.846995  0.863636   0.833856  0.895623   \n",
       "14              0.971897             0.841530  0.857610   0.834395  0.882155   \n",
       "15              0.915691             0.826958  0.848485   0.806061  0.895623   \n",
       "16              0.987510             0.825137  0.841060   0.827362  0.855219   \n",
       "17              0.850117             0.799636  0.832827   0.759003  0.922559   \n",
       "18              0.846214             0.797814  0.839363   0.736041  0.976431   \n",
       "19              0.852459             0.795993  0.836735   0.737789  0.966330   \n",
       "20              0.839969             0.790528  0.835007   0.727500  0.979798   \n",
       "21              0.846214             0.788707  0.825826   0.745257  0.925926   \n",
       "22              0.853240             0.788707  0.823708   0.750693  0.912458   \n",
       "23              0.803279             0.726776  0.795640   0.668192  0.983165   \n",
       "24              1.000000             0.690346  0.769022   0.644647  0.952862   \n",
       "25              1.000000             0.681239  0.753868   0.647343  0.902357   \n",
       "26              0.701015             0.661202  0.760925   0.615385  0.996633   \n",
       "27              0.663544             0.597450  0.728834   0.573359  1.000000   \n",
       "28              1.000000             0.564663  0.708181   0.555556  0.976431   \n",
       "\n",
       "     ROC-AUC  \n",
       "0   0.959476  \n",
       "1   0.951659  \n",
       "2   0.942701  \n",
       "3   0.926347  \n",
       "4   0.914021  \n",
       "5   0.909452  \n",
       "6   0.902116  \n",
       "7   0.894601  \n",
       "8   0.855399  \n",
       "9   0.846080  \n",
       "10  0.842112  \n",
       "11  0.846320  \n",
       "12  0.846320  \n",
       "13  0.842653  \n",
       "14  0.837903  \n",
       "15  0.820827  \n",
       "16  0.822451  \n",
       "17  0.788660  \n",
       "18  0.781866  \n",
       "19  0.780784  \n",
       "20  0.773629  \n",
       "21  0.776455  \n",
       "22  0.777658  \n",
       "23  0.703884  \n",
       "24  0.666907  \n",
       "25  0.661496  \n",
       "26  0.631253  \n",
       "27  0.561508  \n",
       "28  0.527898  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "As can be seen from the results table above, we have evaluated each model on 6 metrics:\n",
    "1. Train Accuracy Score: Accuracy of model at predicting train dataset\n",
    "2. Test Accuracy Score: Accuracy of model at predicting test dataset (**Main Criteria**)\n",
    "3. F1 Score: Weighted average of precision and recall\n",
    "4. Precision: Positive Predictive Value (True Positives / Total Positive Predictions)\n",
    "5. Recall: Sensitivity or True Positive Rate (True Positives / Total Actual Positives)\n",
    "6. ROC-AUC Score: Area under the Receiver Operating Characteristic Curve from prediction scores\n",
    "\n",
    "In the context of this project, we are dealing with a binary classification problem between two subreddits of equal importance to us. Thus, a positive or negative prediction does not really matter, as long as it is correct. Thus, we will be focusing on the accuracy score and ROC-AUC score from here onwards.\n",
    "\n",
    "### Vectorizer Average Scores\n",
    "\n",
    "| Vectorizer         | Train Accuracy Score | Test Accuracy Score | ROC-AUC |\n",
    "|--------------------|----------------------|---------------------|---------|\n",
    "| Tfidf Vectorizer   | 0.942                | 0.842               | 0.833   |\n",
    "| Count Vectorizer   | 0.940                | 0.830               | 0.821   |\n",
    "| Hashing Vectorizer | 0.904                | 0.777               | 0.761   |\n",
    "\n",
    "Tfidf Vectorizer generally performed the best, followed closely by Count Vectorizer, then Hashing Vectorizer quite far behind. Hashing Vectorizer probably performed the worse due to its particularly poor performance in conjunction with Random Forest, Extra Trees & KNN (Bottom 3 performing models, wrost one even barely beating the baseline model). Generally, Tfidf & Count Vectorizer performed quite similarly to each other, but the regularisation of overly common words probably helped improved the performance for Tfidf Vectorizer models slightly more.\n",
    "\n",
    "### Classifier Average Scores\n",
    "\n",
    "| Classifier                    |          | Train Accuracy Score | Test Accuracy Score | ROC-AUC |\n",
    "|-------------------------------|----------|----------------------|---------------------|---------|\n",
    "| Multinomial Nave Bayes       | `mnb`    | 0.992                | 0.954               | 0.956   |\n",
    "| Support Vector Classification | `svc`    | 0.995                | 0.923               | 0.921   |\n",
    "| Logistic Regression           | `logreg` | 0.988                | 0.913               | 0.909   |\n",
    "| Gradient Boost                | `gb`     | 0.978                | 0.856               | 0.848   |\n",
    "| Bagging                       | `bag`    | 0.987                | 0.846               | 0.842   |\n",
    "| Ada Boost                     | `ada`    | 0.960                | 0.834               | 0.830   |\n",
    "| Decision Tree                 | `dt`     | 0.850                | 0.792               | 0.781   |\n",
    "| Random Forest                 | `rf`     | 0.800                | 0.752               | 0.731   |\n",
    "| Extra Trees                   | `et`     | 0.769                | 0.705               | 0.680   |\n",
    "| K Nearest Neighbors           | `knn`    | 1.000                | 0.645               | 0.619   |\n",
    "\n",
    "Most of the models actually performed quite well, with 3 models even achieving >90% accuracy scores. Some notable observations:\n",
    "- `knn` actually had perfect train accuracy scores with all 3 vectorizers, but the worst test accuracy score, thus indicating a very low bias but high variance (i.e. an overfit model)\n",
    "- Surprisingly, `dt` performed better than `rf` and `et` in both bias and variance metrics. This could be due to better hyperparameter tuning for `dt`\n",
    "- `mnb`, `svc` and `logreg` all performed exceedingly well on both the train and test sets\n",
    "- ROC-AUC Score had the same rankings as test accuracy score\n",
    "- `gb` and `ada` performd quite well (>80% test accuracy) despite limited model tuning. With greater computation power to tune more hyperparameters, perhaps these classifiers could even outperform the top few"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes vs Support Vector Classification\n",
    "\n",
    "Here, we will be exploring the two best performing models further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Train Accuracy Score</th>\n",
       "      <th>Test Accuracy Score</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.958106</td>\n",
       "      <td>0.959476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(random_state=42)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943534</td>\n",
       "      <td>0.942701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Vectorizer            Classifier  Train Accuracy Score  \\\n",
       "0  TfidfVectorizer()       MultinomialNB()                   1.0   \n",
       "1  TfidfVectorizer()  SVC(random_state=42)                   1.0   \n",
       "\n",
       "   Test Accuracy Score   ROC-AUC  \n",
       "0             0.958106  0.959476  \n",
       "1             0.943534  0.942701  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 2 Models\n",
    "model_results.iloc[[0,2]].drop(columns=['F1', 'Precision', 'Recall']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, both models had perfect train accuracy scores and very high test accuracy scores, as well as ROC-AUC scores. These models perform extremely well in classifying the subreddit posts. Both models implemented the Tfidf Vectorizer with identical hyperparameters (`ngram_range = (1,2)`). `mnb` slightly edges `svc` for both metrics, although the difference is quite small.\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a good way to visualise how many samples from each class got predicted correctly / wrongly. \n",
    "\n",
    "![confusion_matrix](../images/confusion_matrix_explanation.png \"Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of this project, the different outcomes can be explained as such:\n",
    "- **TN**: Belongs to `r/AskSocialScience`, predicted `r/AskSocialScience`\n",
    "- **FP**: Belongs to `r/AskSocialScience`, predicted `r/AskScience`\n",
    "- **FN**: Belongs to `r/AskScience`, predicted `r/AskSocialScience`\n",
    "- **TP**: Belongs to `r/AskScience`, predicted `r/AskScience`\n",
    "\n",
    "As mentioned earlier, for the context of this project, there is no real difference between Type I and Type II error. We are more interested in the actual numbers in each quadrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bag-of-words\n",
    "final_vec = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_tvec = final_vec.fit_transform(X_train)\n",
    "X_test_tvec = final_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.21544346900318823)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best MNB model\n",
    "final_mnb = MultinomialNB(alpha=0.21544346900318823)\n",
    "final_mnb.fit(X_train_tvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best SVC model\n",
    "final_svc = svm.SVC(kernel='linear')\n",
    "final_svc.fit(X_train_tvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting confusion matrices\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,7))\n",
    "ax = ax.ravel()\n",
    "\n",
    "metrics.plot_confusion_matrix(\n",
    "    final_mnb,\n",
    "    X_test_tvec,\n",
    "    y_test,\n",
    "    normalize='true',\n",
    "    cmap='YlGn',\n",
    "    ax=ax[0]\n",
    ")\n",
    "ax[0].set_title('Multinomial NB Confusion Matrix', fontsize=16)\n",
    "\n",
    "metrics.plot_confusion_matrix(\n",
    "    final_svc,\n",
    "    X_test_tvec,\n",
    "    y_test,\n",
    "    normalize='true',\n",
    "    cmap='Blues',\n",
    "    ax=ax[1]\n",
    ")\n",
    "ax[1].set_title('Support Vector Classifier Confusion Matrix', fontsize=16)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the respective confusion matrices, there is not much difference between the two models. `mnb` was better at correctly predicting posts from `r/AskSocialScience`, while `svc` was slightly better at correctly predicting posts from `r/AskScience`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC-AUC Curve\n",
    "\n",
    "We can also plot the ROC-AUC Curve to better visualise how 'separated' the distributions are for each model. The area under the ROC-AUC curve represents the gap between distributions of each subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,7))\n",
    "\n",
    "# Baseline plot (Perfect overlap between both positive & negative distributions)\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, 200),\n",
    "    np.linspace(0, 1, 200),\n",
    "    label='baseline',\n",
    "    linestyle='--'\n",
    ")\n",
    "\n",
    "# MNB ROC Curve\n",
    "metrics.plot_roc_curve(\n",
    "    final_mnb,\n",
    "    X_test_tvec,\n",
    "    y_test,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# SVC ROC Curve\n",
    "metrics.plot_roc_curve(\n",
    "    final_svc,\n",
    "    X_test_tvec,\n",
    "    y_test,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "plt.title('ROC Curve for Multinomial NB & SVC', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from above, once again there is barely any difference between the two models. Both perform extremely well with ROC-AUC scores of 0.99. However, from the plot, we can see that `mnb` performs slightly better at lower decision thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation\n",
    "\n",
    "Another area we can look at is the interpretability of the models. So far, we have been evaluating the predictive value of the models. However, classification models that are good for inference can also prove useful to stakeholders such as the subreddit moderators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes\n",
    "\n",
    "One great aspect of `mnb` is that we are able to extract the feature importance from the model and rank them accordingly for each subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Important Features for r/AskScience:\n",
      "['vaccin' 'covid' 'differ' 'earth' 'viru' 'cell' 'water' 'immun' 'effect'\n",
      " 'peopl' 'possibl' 'make' 'light' 'time' 'human']\n",
      "\n",
      "Top 15 Important Features for r/AskSocialScience:\n",
      "['peopl' 'social' 'studi' 'cultur' 'research' 'countri' 'societi'\n",
      " 'american' 'read' 'think' 'person' 'polit' 'theori' 'book' 'thing']\n"
     ]
    }
   ],
   "source": [
    "# Extracting top 15 important features for each subreddit\n",
    "pos_class_prob_sorted = final_mnb.feature_log_prob_[1, :].argsort()[::-1]\n",
    "neg_class_prob_sorted = final_mnb.feature_log_prob_[0, :].argsort()[::-1]\n",
    "\n",
    "print('Top 15 Important Features for r/AskScience:')\n",
    "print(np.take(final_vec.get_feature_names(), pos_class_prob_sorted[:15]))\n",
    "print('')\n",
    "print('Top 15 Important Features for r/AskSocialScience:')\n",
    "print(np.take(final_vec.get_feature_names(), neg_class_prob_sorted[:15]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we are able to compare the top important features with the most frequent words that we explored during the EDA earlier.\n",
    "\n",
    "| Rank | r/AskScience |                    | r/AskSocialScience |                    |\n",
    "|------|--------------|--------------------|--------------------|--------------------|\n",
    "|      | **Frequency**    | **Feature Importance** | **Frequency**          | **Feature Importance** |\n",
    "| 1    | vaccin       | vaccin             | peopl              | peopl              |\n",
    "| 2    | covid        | covid              | social             | social             |\n",
    "| 3    | differ       | differ             | studi              | studi              |\n",
    "| 4    | peopl        | earth              | cultur             | cultur             |\n",
    "| 5    | make         | viru               | research           | research           |\n",
    "| 6    | time         | cell               | countri            | countri            |\n",
    "| 7    | understand   | water              | differ             | societi            |\n",
    "| 8    | cell         | immun              | time               | american           |\n",
    "| 9    | effect       | effect             | think              | read               |\n",
    "| 10   | work         | peopl              | thing              | think              |\n",
    "| 11   | mean         | possibl            | exampl             | person             |\n",
    "| 12   | viru         | make               | read               | polit              |\n",
    "| 13   | say          | light              | american           | theori             |\n",
    "| 14   | earth        | time               | person             | book               |\n",
    "| 15   | possibl      | human              | make               | thing              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting things to note from the comparison:\n",
    "- Generally, top few most frequent words are still the most predictive\n",
    "- Generic words (e.g. `make`, `understand`, `thing`) tend to drop off in feature importance as compared to their frequency\n",
    "- Some words are still important in predicting both classes despite appearing in both lists (e.g. `peopl`), which suggests that features need not always point to one specific class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier \n",
    "\n",
    "On the other hand, `svc` is generally a black-box model, especially when implementing non-linear kernels such as polynomial and gaussian. It is difficult to interpret the coefficients from the model, which makes it worse at inference despite its good predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1574135580858934,\n",
       " 0.05035385805822142,\n",
       " 0.03592094998174063,\n",
       " 0.03592094998174063,\n",
       " 0.03592094998174063,\n",
       " 0.06233297046146063,\n",
       " -0.04458539819208144,\n",
       " 0.3238225012413873,\n",
       " 0.3190075428948135,\n",
       " 0.0,\n",
       " -0.01728401698331917,\n",
       " -0.08080715576581932,\n",
       " 0.0009023905349321639,\n",
       " 0.0018047810698643277,\n",
       " 0.0,\n",
       " -0.019883999230064876,\n",
       " 0.057333302951536456,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.13266850421854717,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01204769770059163,\n",
       " 0.01204769770059163,\n",
       " 0.05035385805822142,\n",
       " 0.05035385805822142,\n",
       " 0.07096474307044069,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07504174836558415,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02330846161354894,\n",
       " 0.02330846161354894,\n",
       " 0.16734772845204057,\n",
       " -0.03514868427866273,\n",
       " 0.0,\n",
       " -0.029111647102249956,\n",
       " -0.017023942057793867,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1378732533387008,\n",
       " 0.0,\n",
       " 0.04291673346399574,\n",
       " -0.007330278875334225,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08360477585544351,\n",
       " -0.05289655563353358,\n",
       " -0.04751820394431691,\n",
       " 0.0,\n",
       " 0.08766863619167965,\n",
       " 0.0,\n",
       " 0.0552736139153143,\n",
       " 0.0,\n",
       " -0.020427302542327285,\n",
       " -0.020427302542327285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.10720819657888839,\n",
       " 0.04373205117249531,\n",
       " 0.051605018494691485,\n",
       " 0.08553220715648902,\n",
       " 0.39701474250189517,\n",
       " 0.057333302951536456,\n",
       " 0.14368128690624338,\n",
       " -0.007556268351637164,\n",
       " 0.08720603248088783,\n",
       " 0.005906154902921334,\n",
       " 0.0,\n",
       " 0.04104303201785903,\n",
       " 0.03922428685693846,\n",
       " 0.028323426089316327,\n",
       " 0.0552736139153143,\n",
       " 0.0,\n",
       " -0.029573712267578843,\n",
       " 0.07702394826858117,\n",
       " -0.007556268351637164,\n",
       " 0.03922428685693846,\n",
       " 0.09736562910915812,\n",
       " 0.04104303201785903,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.017023942057793867,\n",
       " 0.0,\n",
       " 0.00656844354483731,\n",
       " -0.1238277142697595,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06708876467064957,\n",
       " 0.07871730013669312,\n",
       " -0.0705162344215198,\n",
       " 0.004957806694181235,\n",
       " 0.09596671932132912,\n",
       " -0.09072408098723889,\n",
       " 0.30547206863912835,\n",
       " 0.07636801715978209,\n",
       " 0.22910405147934626,\n",
       " 0.09707939639590261,\n",
       " 0.09707939639590261,\n",
       " 0.02039623833382285,\n",
       " 0.02039623833382285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.020703829259444716,\n",
       " 0.020703829259444716,\n",
       " 0.3183376804649909,\n",
       " 0.3183376804649909,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.017535854341181645,\n",
       " 0.017535854341181645,\n",
       " 0.16939939158406309,\n",
       " 0.09596671932132912,\n",
       " 0.08316486020001383,\n",
       " -0.07371284397724359,\n",
       " 0.01204769770059163,\n",
       " -0.15116469162437685,\n",
       " 0.0,\n",
       " 0.029371532438556003,\n",
       " 0.02330846161354894,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05190162664489032,\n",
       " 0.05190162664489032,\n",
       " -0.029111647102249956,\n",
       " -0.029111647102249956,\n",
       " 0.07686057583929427,\n",
       " 0.07686057583929427,\n",
       " 0.05190162664489032,\n",
       " 0.05190162664489032,\n",
       " 0.15206948496257183,\n",
       " 0.05035385805822142,\n",
       " -0.05857156722950302,\n",
       " 0.04291673346399574,\n",
       " 0.051605018494691485,\n",
       " 0.0,\n",
       " 0.0759713001108989,\n",
       " -0.044212255159983144,\n",
       " 0.07169855451559556,\n",
       " 0.11634650852935213,\n",
       " 0.046170172085877016,\n",
       " 0.07686057583929427,\n",
       " 0.11244496315996326,\n",
       " 0.05622248157998163,\n",
       " 0.05622248157998163,\n",
       " -0.01447319788278541,\n",
       " -0.01447319788278541,\n",
       " 0.2879001579639874,\n",
       " 0.19193343864265824,\n",
       " 0.09596671932132912,\n",
       " -0.07212125231740622,\n",
       " -0.017023942057793867,\n",
       " -0.06871249945344624,\n",
       " 0.03592094998174063,\n",
       " 0.028567322382818883,\n",
       " 0.0,\n",
       " -0.05289655563353358,\n",
       " -0.007654423017529277,\n",
       " 0.0,\n",
       " -0.00852596726984919,\n",
       " -0.017023942057793867,\n",
       " -0.017023942057793867,\n",
       " -0.00286783074218033,\n",
       " -0.00286783074218033,\n",
       " -0.00286783074218033,\n",
       " -0.00286783074218033,\n",
       " 0.06402582021483896,\n",
       " 0.06402582021483896,\n",
       " 0.12530047183119278,\n",
       " 0.06265023591559639,\n",
       " 0.06265023591559639,\n",
       " -0.031583604361364974,\n",
       " -0.031583604361364974,\n",
       " 0.2585000967008934,\n",
       " 0.07184189996348125,\n",
       " 0.08013964470962091,\n",
       " 0.026233039357936503,\n",
       " 0.0,\n",
       " 0.06134121192241883,\n",
       " 0.05190162664489032,\n",
       " -0.019530044441649044,\n",
       " -0.10615640453942869,\n",
       " 0.16027928941924183,\n",
       " 0.0407924766676457,\n",
       " 0.02039623833382285,\n",
       " 0.02039623833382285,\n",
       " -0.17298997014067774,\n",
       " 0.0,\n",
       " -0.017023942057793867,\n",
       " -0.08723439782174898,\n",
       " -0.017023942057793867,\n",
       " 0.03989676813209496,\n",
       " -0.017023942057793867,\n",
       " 0.0,\n",
       " 0.023358771595100414,\n",
       " 0.02145836673199787,\n",
       " 0.0,\n",
       " -0.017023942057793867,\n",
       " -0.053078202269714346,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.10720819657888839,\n",
       " 0.0,\n",
       " 0.20134844849443279,\n",
       " 0.0977310131120692,\n",
       " 0.0,\n",
       " 0.12463785961031619,\n",
       " -0.031583604361364974,\n",
       " -0.031583604361364974,\n",
       " -0.034781079078301434,\n",
       " -0.034781079078301434,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.014061525602746313,\n",
       " -0.017023942057793867,\n",
       " 0.0,\n",
       " -0.06261638506724965,\n",
       " -0.05289655563353358,\n",
       " 0.02330846161354894,\n",
       " 0.04661692322709788,\n",
       " 0.02145836673199787,\n",
       " 0.02330846161354894,\n",
       " -0.06261638506724965,\n",
       " 0.09274357475627158,\n",
       " 0.05622248157998163,\n",
       " 0.05622248157998163,\n",
       " 0.03128685178802019,\n",
       " 0.03128685178802019,\n",
       " -0.029111647102249956,\n",
       " -0.029111647102249956,\n",
       " 0.08712979689485953,\n",
       " 0.0,\n",
       " -0.031583604361364974,\n",
       " 0.05035385805822142,\n",
       " -0.06261638506724965,\n",
       " 0.061256440556570806,\n",
       " -0.017023942057793867,\n",
       " 0.12973066879007694,\n",
       " -0.020995124173110082,\n",
       " 0.0407924766676457,\n",
       " 0.02039623833382285,\n",
       " 0.02039623833382285,\n",
       " 0.059697993878845196,\n",
       " 0.05622248157998163,\n",
       " 0.06265023591559639,\n",
       " 0.05847049951738508,\n",
       " 0.0,\n",
       " -0.053970603954525825,\n",
       " 0.05622248157998163,\n",
       " -0.06261638506724965,\n",
       " -0.06871249945344624,\n",
       " 0.04989919984575007,\n",
       " -0.024715682761746394,\n",
       " -0.019592660505349534,\n",
       " -0.019592660505349534,\n",
       " 0.04661692322709788,\n",
       " 0.02330846161354894,\n",
       " 0.02330846161354894,\n",
       " 0.03128685178802019,\n",
       " 0.03128685178802019,\n",
       " -0.007556268351637164,\n",
       " -0.007556268351637164,\n",
       " -0.034047884115587734,\n",
       " -0.017023942057793867,\n",
       " -0.017023942057793867,\n",
       " 0.05622248157998163,\n",
       " 0.05622248157998163,\n",
       " 0.5532489612072623,\n",
       " 0.029371532438556003,\n",
       " 0.05631162149416238,\n",
       " 0.1328973711105537,\n",
       " 0.11545684421192472,\n",
       " 0.0651587868461878,\n",
       " 0.0,\n",
       " 0.058779979094510104,\n",
       " -0.14804046064238466,\n",
       " -0.1295035795823808,\n",
       " 0.007410986098977548,\n",
       " 0.03315187583130562,\n",
       " 0.05847049951738508,\n",
       " -0.03289142902800927,\n",
       " -0.08344590928966465,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06253745898319649,\n",
       " 0.014441679623717467,\n",
       " 0.0,\n",
       " -0.07402023032119233,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.13881839589037182,\n",
       " 0.0004350114683847979,\n",
       " -0.2385011607992668,\n",
       " 0.11545684421192472,\n",
       " 0.007410986098977548,\n",
       " -0.08350172707208453,\n",
       " 0.1641721280714361,\n",
       " 0.06708719668745572,\n",
       " 0.0,\n",
       " 0.05847049951738508,\n",
       " 0.04268906732188986,\n",
       " 0.05529380732491757,\n",
       " -0.07888225611264564,\n",
       " 0.028982234346441926,\n",
       " 0.10711619771914652,\n",
       " 0.010370940676540392,\n",
       " 0.009807489251503156,\n",
       " 0.11079102113569989,\n",
       " 0.32458143092831043,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.036011831500856885,\n",
       " 0.03128685178802019,\n",
       " 0.03128685178802019,\n",
       " 0.09386055536406057,\n",
       " 0.03128685178802019,\n",
       " 0.03128685178802019,\n",
       " 0.03128685178802019,\n",
       " 0.03989676813209496,\n",
       " 0.03989676813209496,\n",
       " 0.05880995777604523,\n",
       " 0.05880995777604523,\n",
       " -0.0510718261733816,\n",
       " -0.017023942057793867,\n",
       " -0.017023942057793867,\n",
       " -0.017023942057793867,\n",
       " 0.02409539540118326,\n",
       " 0.02409539540118326,\n",
       " -0.024240114674422143,\n",
       " -0.024240114674422143,\n",
       " -0.020995124173110082,\n",
       " -0.020995124173110082,\n",
       " -0.1403369924927509,\n",
       " -0.017023942057793867,\n",
       " -0.017023942057793867,\n",
       " -0.08791180439778194,\n",
       " -0.017023942057793867,\n",
       " -0.020995124173110082,\n",
       " -0.004889933762255361,\n",
       " -0.004889933762255361,\n",
       " -0.024240114674422143,\n",
       " -0.024240114674422143,\n",
       " 0.016544859252608124,\n",
       " -0.024281125504336885,\n",
       " -0.023693820549868856,\n",
       " 0.06621378196335792,\n",
       " -0.07287076422907054,\n",
       " -0.03643538211453527,\n",
       " -0.03643538211453527,\n",
       " -0.049722135247193947,\n",
       " -0.031583604361364974,\n",
       " -0.020995124173110082,\n",
       " -0.1308644120464473,\n",
       " -0.1308644120464473,\n",
       " -0.14258515988410742,\n",
       " -0.14258515988410742,\n",
       " -0.19069562420228606,\n",
       " -0.020995124173110082,\n",
       " -0.09032809038992538,\n",
       " -0.09032809038992538,\n",
       " 0.05190162664489032,\n",
       " 0.05190162664489032,\n",
       " 0.003952829201069416,\n",
       " 0.046170172085877016,\n",
       " -0.041990248346220164,\n",
       " -0.007654423017529277,\n",
       " -0.007654423017529277,\n",
       " 0.10380325328978064,\n",
       " 0.05190162664489032,\n",
       " 0.05190162664489032,\n",
       " -0.007654423017529277,\n",
       " -0.007654423017529277,\n",
       " 0.05190162664489032,\n",
       " 0.05190162664489032,\n",
       " -0.007654423017529277,\n",
       " -0.007654423017529277,\n",
       " -0.05476530707082839,\n",
       " 0.046170172085877016,\n",
       " -0.052040904203892846,\n",
       " -0.052040904203892846,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.17034979589920346,\n",
       " 0.0553639480018814,\n",
       " 0.06940868964282651,\n",
       " 0.0553639480018814,\n",
       " 0.2354225933676126,\n",
       " 0.2354225933676126,\n",
       " -0.1676909704559049,\n",
       " -0.03167412465180147,\n",
       " -0.09502237395540442,\n",
       " -0.021728446398273357,\n",
       " -0.03643538211453527,\n",
       " 0.20226409148790092,\n",
       " 0.03592094998174063,\n",
       " 0.07504174836558415,\n",
       " 0.02239905188693241,\n",
       " -0.007330278875334225,\n",
       " -0.09559269772529877,\n",
       " 0.07200487482851328,\n",
       " -0.001571408821038775,\n",
       " -0.053078202269714346,\n",
       " 0.06036072338911231,\n",
       " -0.017023942057793867,\n",
       " -0.02910758678972448,\n",
       " -0.1045770358771512,\n",
       " -0.007556268351637164,\n",
       " 0.046170172085877016,\n",
       " -0.020995124173110082,\n",
       " 0.0759713001108989,\n",
       " 0.08720603248088783,\n",
       " 0.015484726327922442,\n",
       " 0.05876845244355568,\n",
       " 0.028323426089316327,\n",
       " 0.1706914422449275,\n",
       " 0.05035385805822142,\n",
       " -0.1290178395550015,\n",
       " 0.10375328601640783,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.007556268351637164,\n",
       " 0.01204769770059163,\n",
       " 0.0901983169096107,\n",
       " 0.0299931206096978,\n",
       " -0.03543113752356361,\n",
       " -0.017023942057793867,\n",
       " 0.046170172085877016,\n",
       " -0.017023942057793867,\n",
       " -0.053970603954525825,\n",
       " -0.053970603954525825,\n",
       " -0.053970603954525825,\n",
       " 0.10939939467995474,\n",
       " -0.14091971066878736,\n",
       " -0.07116067377227502,\n",
       " -0.08418736757464962,\n",
       " 0.0,\n",
       " 0.030691157608961545,\n",
       " 0.03383352973543944,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.03710623209321692,\n",
       " -0.007654423017529277,\n",
       " -0.031583604361364974,\n",
       " 0.08378292200948656,\n",
       " 0.05476281654497287,\n",
       " 0.03383352973543944,\n",
       " 0.101216694938161,\n",
       " 0.017535854341181645,\n",
       " -0.031583604361364974,\n",
       " 0.12562770739895757,\n",
       " 0.05190162664489032,\n",
       " 0.05190162664489032,\n",
       " -0.027517644229629545,\n",
       " -0.024281125504336885,\n",
       " -0.024281125504336885,\n",
       " -0.00286783074218033,\n",
       " 0.021094991554693982,\n",
       " -0.05545996819632697,\n",
       " -0.024281125504336885,\n",
       " -0.03436508101572321,\n",
       " 0.14368128690624338,\n",
       " 0.07184064345312169,\n",
       " 0.07184064345312169,\n",
       " 0.029664920778518997,\n",
       " 0.017535854341181645,\n",
       " -0.0037025044264281464,\n",
       " 0.017535854341181645,\n",
       " 0.005571712820922753,\n",
       " -0.017023942057793867,\n",
       " 0.05190162664489032,\n",
       " -0.009979981662371268,\n",
       " -0.034781079078301434,\n",
       " -0.017023942057793867,\n",
       " 0.045093287746572897,\n",
       " -0.007654423017529277,\n",
       " 0.045093287746572897,\n",
       " -0.007654423017529277,\n",
       " 0.1404404496382357,\n",
       " 0.046170172085877016,\n",
       " -0.009979981662371268,\n",
       " 0.07200487482851328,\n",
       " 0.05190162664489032,\n",
       " 0.00923901380971718,\n",
       " -0.007654423017529277,\n",
       " -0.024281125504336885,\n",
       " -0.0037025044264281464,\n",
       " 0.046170172085877016,\n",
       " -0.019937640962476785,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.013399252896089685,\n",
       " -0.009979981662371268,\n",
       " -0.09061214061129126,\n",
       " -0.041127615526235,\n",
       " -0.007654423017529277,\n",
       " -0.009979981662371268,\n",
       " -0.041127615526235,\n",
       " -0.36234244615492994,\n",
       " -0.05754743136090098,\n",
       " -0.007330278875334225,\n",
       " -0.4024597640302297,\n",
       " -0.007654423017529277,\n",
       " 0.10380325328978064,\n",
       " 0.0,\n",
       " -0.023195086918368732,\n",
       " -0.0379230411385413,\n",
       " -0.05593763114512898,\n",
       " -0.041127615526235,\n",
       " -0.007654423017529277,\n",
       " -0.007330278875334225,\n",
       " -0.007654423017529277,\n",
       " 0.0,\n",
       " -0.08080556932422933,\n",
       " 0.0,\n",
       " -0.06123540541198077,\n",
       " -0.016099034210821678,\n",
       " -0.029111647102249956,\n",
       " 0.0,\n",
       " -0.017023942057793867,\n",
       " -0.007654423017529277,\n",
       " 0.05190162664489032,\n",
       " -0.013399252896089685,\n",
       " 0.0,\n",
       " -0.007654423017529277,\n",
       " 0.0,\n",
       " 0.06567013439180797,\n",
       " 0.05847049951738508,\n",
       " -0.03643538211453527,\n",
       " 0.0,\n",
       " -0.03643538211453527,\n",
       " 0.05847049951738508,\n",
       " -0.041127615526235,\n",
       " -0.03643538211453527,\n",
       " 0.05847049951738508,\n",
       " 0.0299931206096978,\n",
       " 0.05847049951738508,\n",
       " -0.03643538211453527,\n",
       " 0.02763888456690946,\n",
       " -0.020995124173110082,\n",
       " 0.12562770739895757,\n",
       " 0.2354225933676126,\n",
       " 0.05190162664489032,\n",
       " -0.0066672070307168835,\n",
       " -0.11074454415526332,\n",
       " 0.0,\n",
       " -0.013477573893570915,\n",
       " -0.05418638978966317,\n",
       " 0.05847049951738508,\n",
       " -0.25230076067820206,\n",
       " 0.05190162664489032,\n",
       " -0.02806076633115061,\n",
       " -0.04282881903292948,\n",
       " 0.0,\n",
       " 0.05190162664489032,\n",
       " -0.04955777759448135,\n",
       " -0.08350172707208453,\n",
       " -0.08350172707208453,\n",
       " 0.025234606036330088,\n",
       " 0.025234606036330088,\n",
       " -0.11203794203745077,\n",
       " -0.11203794203745077,\n",
       " -0.09460758804223202,\n",
       " -0.09460758804223202,\n",
       " 0.10200469793243339,\n",
       " 0.026233039357936503,\n",
       " 0.05622248157998163,\n",
       " 0.0299931206096978,\n",
       " -0.08821728868502092,\n",
       " -0.08821728868502092,\n",
       " -0.15536202757559672,\n",
       " -0.15536202757559672,\n",
       " -0.03913180669615251,\n",
       " -0.03913180669615251,\n",
       " 0.04683546503856503,\n",
       " -0.017023942057793867,\n",
       " 0.0,\n",
       " 0.06658895326802104,\n",
       " 0.01204769770059163,\n",
       " 0.01204769770059163,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.001723023803150211,\n",
       " -0.001723023803150211,\n",
       " 0.04004288857413256,\n",
       " -0.007556268351637164,\n",
       " -0.017023942057793867,\n",
       " -0.017023942057793867,\n",
       " 0.03526933861380101,\n",
       " 0.05190162664489032,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.38977429091854315,\n",
       " 0.0009023905349321639,\n",
       " 0.029061773731717276,\n",
       " -0.007330278875334225,\n",
       " 0.05190162664489032,\n",
       " 0.02330846161354894,\n",
       " 0.15709372748745803,\n",
       " 0.07058057859447074,\n",
       " 0.0,\n",
       " 0.11529779842609558,\n",
       " 0.02330846161354894,\n",
       " 0.04104303201785903,\n",
       " 0.07979353626418992,\n",
       " 0.03989676813209496,\n",
       " 0.03989676813209496,\n",
       " 0.06402582021483896,\n",
       " 0.06402582021483896,\n",
       " -0.016099034210821678,\n",
       " -0.017023942057793867,\n",
       " 0.0,\n",
       " 0.08230907199107165,\n",
       " 0.05847049951738508,\n",
       " 0.028567322382818883,\n",
       " -0.03185802467255142,\n",
       " -0.07116067377227502,\n",
       " 0.03747236930636371,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.007238559530158197,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.007654423017529277,\n",
       " -0.14139682038082796,\n",
       " -0.14139682038082796,\n",
       " 0.22894713563864294,\n",
       " 0.22894713563864294,\n",
       " -0.03445585406452434,\n",
       " 0.0,\n",
       " -0.03643538211453527,\n",
       " -0.24722061275807317,\n",
       " -0.017023942057793867,\n",
       " -0.021728446398273357,\n",
       " -0.0037025044264281464,\n",
       " -0.03643538211453527,\n",
       " -0.09559269772529877,\n",
       " -0.06650704528334356,\n",
       " -0.052040904203892846,\n",
       " -0.021728446398273357,\n",
       " -0.005126411129674421,\n",
       " -0.09827825350372923,\n",
       " -0.09827825350372923,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02330846161354894,\n",
       " 0.02330846161354894,\n",
       " 0.09211249759014807,\n",
       " 0.09211249759014807,\n",
       " 0.02039623833382285,\n",
       " 0.02039623833382285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0552736139153143,\n",
       " 0.0552736139153143,\n",
       " 0.05230090572238835,\n",
       " 0.05230090572238835,\n",
       " 0.05230090572238835,\n",
       " 0.05230090572238835,\n",
       " -0.050022694854569186,\n",
       " 0.0,\n",
       " -0.05289655563353358,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.024281125504336885,\n",
       " -0.024281125504336885,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.00455097887242676,\n",
       " 0.03989676813209496,\n",
       " -0.02806076633115061,\n",
       " -0.017023942057793867,\n",
       " 0.0,\n",
       " 0.011221097822927094,\n",
       " 0.005610548911463547,\n",
       " 0.005610548911463547,\n",
       " -0.0495942000109853,\n",
       " -0.007556268351637164,\n",
       " -0.007654423017529277,\n",
       " -0.007654423017529277,\n",
       " 0.01204769770059163,\n",
       " -0.08723439782174898,\n",
       " 0.03989676813209496,\n",
       " 0.029061773731717276,\n",
       " 0.029061773731717276,\n",
       " -0.14505808066356812,\n",
       " 0.007410986098977548,\n",
       " -0.013477573893570915,\n",
       " -0.1538435383735888,\n",
       " -0.10595093989674431,\n",
       " 0.0,\n",
       " -0.11203794203745077,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05880995777604523,\n",
       " 0.05880995777604523,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.11991322629053507,\n",
       " 0.09158133688798574,\n",
       " 0.03522104079507916,\n",
       " 0.02039623833382285,\n",
       " 0.02039623833382285,\n",
       " -0.03117333975927224,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.03436508101572321,\n",
       " 0.20152571992666113,\n",
       " -0.08821728868502092,\n",
       " 0.07807419379027573,\n",
       " 0.1869623935874911,\n",
       " -0.0037025044264281464,\n",
       " 0.07200487482851328,\n",
       " -0.06404355902380322,\n",
       " -0.08080715576581932,\n",
       " 0.0759713001108989,\n",
       " 0.08069103134607307,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.06369803474867555,\n",
       " -0.007556268351637164,\n",
       " -0.04332858626034851,\n",
       " -0.021728446398273357,\n",
       " 0.0,\n",
       " 0.07950058040274494,\n",
       " 0.08406797927217,\n",
       " 0.0,\n",
       " 0.08169003838418284,\n",
       " 0.08638322410772159,\n",
       " 0.0,\n",
       " -0.053970603954525825,\n",
       " -0.053970603954525825,\n",
       " -0.053970603954525825,\n",
       " -0.053970603954525825,\n",
       " 0.05876845244355568,\n",
       " 0.05876845244355568,\n",
       " -0.008364572504287113,\n",
       " 0.0,\n",
       " 0.05152529992610034,\n",
       " 0.0,\n",
       " 0.03989676813209496,\n",
       " -0.013477573893570915,\n",
       " 0.0,\n",
       " -0.08821728868502092,\n",
       " -0.007556268351637164,\n",
       " -0.007556268351637164,\n",
       " -0.279912111602779,\n",
       " -0.279912111602779,\n",
       " -0.05857156722950302,\n",
       " -0.05857156722950302,\n",
       " -0.30271835603892494,\n",
       " -0.279912111602779,\n",
       " -0.013399252896089685,\n",
       " -0.013399252896089685,\n",
       " -0.013399252896089685,\n",
       " 0.03299178801592928,\n",
       " 0.03299178801592928,\n",
       " 0.004826190081286036,\n",
       " 0.004826190081286036,\n",
       " -0.08350172707208453,\n",
       " -0.08350172707208453,\n",
       " -0.15947544948203257,\n",
       " -0.08431874782270525,\n",
       " 0.0,\n",
       " -0.08431874782270525,\n",
       " 0.05970698974640133,\n",
       " 0.05970698974640133,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1337433333094205,\n",
       " 0.1337433333094205,\n",
       " 0.053945816314173806,\n",
       " -0.006400622608604015,\n",
       " -0.09559269772529877,\n",
       " -0.1415075521181057,\n",
       " 0.09255348772556243,\n",
       " 0.0977310131120692,\n",
       " -0.006400622608604015,\n",
       " 0.1378732533387008,\n",
       " -0.017023942057793867,\n",
       " 0.04865005289266688,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1869623935874911,\n",
       " -0.019883999230064876,\n",
       " 0.06156856874381565,\n",
       " -0.08431874782270525,\n",
       " -0.15817336532942394,\n",
       " -0.02868696964736925,\n",
       " 0.03299178801592928,\n",
       " -0.06461592722078335,\n",
       " 0.0,\n",
       " -0.086435448648553,\n",
       " -0.086435448648553,\n",
       " 0.060904923677919644,\n",
       " 0.060904923677919644,\n",
       " 0.03299178801592928,\n",
       " 0.03299178801592928,\n",
       " 0.18422499518029614,\n",
       " 0.09211249759014807,\n",
       " 0.024292858732130145,\n",
       " 0.024292858732130145,\n",
       " -0.08730948170589743,\n",
       " -0.007556268351637164,\n",
       " -0.08431874782270525,\n",
       " -0.007654423017529277,\n",
       " 0.0,\n",
       " 0.11880237564113323,\n",
       " 0.0,\n",
       " 0.12562770739895757,\n",
       " -0.08431874782270525,\n",
       " -0.08431874782270525,\n",
       " 0.03652835792164019,\n",
       " 0.03652835792164019,\n",
       " 0.0007678418805634468,\n",
       " -0.0037025044264281464,\n",
       " 0.08638322410772159,\n",
       " -0.017023942057793867,\n",
       " 0.08638322410772159,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2747756579094387,\n",
       " 0.2747756579094387,\n",
       " 0.03299178801592928,\n",
       " 0.03299178801592928,\n",
       " -0.024281125504336885,\n",
       " -0.024281125504336885,\n",
       " -0.024640912811209753,\n",
       " 0.06940868964282651,\n",
       " -0.0954652509530205,\n",
       " 0.051605018494691485,\n",
       " 0.051605018494691485,\n",
       " -0.5095359891151842,\n",
       " -0.08791180439778194,\n",
       " -0.06420121025709562,\n",
       " -0.09559269772529877,\n",
       " 0.0,\n",
       " 0.027669243734166467,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1533479853086603,\n",
       " -0.177660130853399,\n",
       " 0.0,\n",
       " -0.09559269772529877,\n",
       " -0.017023942057793867,\n",
       " 0.09845590805381293,\n",
       " 0.09845590805381293,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3183376804649909,\n",
       " 0.15916884023249545,\n",
       " 0.09211249759014807,\n",
       " 0.09211249759014807,\n",
       " 0.14949712238680735,\n",
       " -0.052040904203892846,\n",
       " 0.21012680650039034,\n",
       " 0.046170172085877016,\n",
       " 0.046170172085877016,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.031583604361364974,\n",
       " -0.031583604361364974,\n",
       " -0.007556268351637164,\n",
       " -0.007556268351637164,\n",
       " -0.11714313445900604,\n",
       " -0.05857156722950302,\n",
       " -0.05857156722950302,\n",
       " 0.0014877903666184883,\n",
       " -0.017023942057793867,\n",
       " -0.024281125504336885,\n",
       " -0.024281125504336885,\n",
       " 0.121032847002811,\n",
       " -0.05375063034381519,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.016099034210821678,\n",
       " 0.0,\n",
       " -0.017023942057793867,\n",
       " -0.06253512163600067,\n",
       " -0.05857156722950302,\n",
       " -0.007556268351637164,\n",
       " -0.07363143155432755,\n",
       " -0.007556268351637164,\n",
       " 0.0,\n",
       " -0.017023942057793867,\n",
       " 0.06036072338911231,\n",
       " -0.08791180439778194,\n",
       " 0.1650039600088834,\n",
       " 0.007410986098977548,\n",
       " 0.007410986098977548,\n",
       " -0.0617255177205567,\n",
       " 0.0009023905349321639,\n",
       " -0.03643538211453527,\n",
       " 0.026861931599047022,\n",
       " -0.05375063034381519,\n",
       " -0.007556268351637164,\n",
       " -0.09559269772529877,\n",
       " -0.2482914252521353,\n",
       " -0.024281125504336885,\n",
       " -0.2382749248516597,\n",
       " 0.060904923677919644,\n",
       " 0.060904923677919644,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05876845244355568,\n",
       " 0.05876845244355568,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.08080715576581932,\n",
       " -0.08080715576581932,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.02197032847985566,\n",
       " -0.0037025044264281464,\n",
       " -0.019530044441649044,\n",
       " 0.21464281515463426,\n",
       " -0.024281125504336885,\n",
       " 0.25125541479791513,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.021728446398273357,\n",
       " -0.021728446398273357,\n",
       " 0.13446271417664768,\n",
       " 0.0,\n",
       " 0.05230090572238835,\n",
       " 0.0,\n",
       " -0.12357516385468091,\n",
       " -0.0037025044264281464,\n",
       " 0.1650039600088834,\n",
       " -0.017023942057793867,\n",
       " 0.0,\n",
       " -0.013399252896089685,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.11466660590307291,\n",
       " 0.047081135665584375,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05190162664489032,\n",
       " 0.049459406500752826,\n",
       " 0.05230090572238835,\n",
       " 0.0,\n",
       " 0.09251763301783514,\n",
       " 0.12562770739895757,\n",
       " -0.02779483101677397,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02039623833382285,\n",
       " 0.02039623833382285,\n",
       " -0.052040904203892846,\n",
       " -0.052040904203892846,\n",
       " -0.031583604361364974,\n",
       " -0.031583604361364974,\n",
       " -0.024281125504336885,\n",
       " -0.024281125504336885,\n",
       " 0.0881938882782971,\n",
       " 0.0881938882782971,\n",
       " 0.07184379242914665,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0759713001108989,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02330846161354894,\n",
       " 0.02330846161354894,\n",
       " -0.021728446398273357,\n",
       " -0.021728446398273357,\n",
       " 0.03796376116527161,\n",
       " 0.03796376116527161,\n",
       " 0.16435119928903272,\n",
       " -0.017023942057793867,\n",
       " -0.052040904203892846,\n",
       " 0.04373205117249531,\n",
       " 0.0469042229733775,\n",
       " 0.17623692183632356,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of coefficients for SVC model\n",
    "list(final_svc.coef_.toarray()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a general kernel, it is difficult to interpret the SVM weights. However, in the case of our two models (linear SVC), there actually is a useful interpretation:\n",
    "\n",
    ">The coefficients represent the coordinates of a vector which is orthogonal to the hyperplane which separates the classes as best as possible.\n",
    ">\n",
    ">With this vector, we can take its dot product with any new point and interpret it as such: If the dot product is positive, it belongs to the positive class, if it is negative it belongs to the negative class.\n",
    ">\n",
    ">The importance of each feature can also be estimated by studying the absolute size of its coefficient relative to all the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model\n",
    "\n",
    "By all comparisons, `mnb` with `Tfidf Vectorizer` edges out `svc` as the better model, both for predictive performance & inferential value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Limitations\n",
    "\n",
    "Despite the great scores, no model is perfect. We will evaluate our model to see which areas are lacking.\n",
    "\n",
    "### Misclassified Posts\n",
    "\n",
    "We can examine misclassified posts to find any weaknesses in our model. We will use `mnb` for these final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing predicted classifications to actual subreddits\n",
    "model_check = pd.DataFrame(final_mnb.predict(X_test_tvec), columns=['pred'])\n",
    "model_preds = pd.DataFrame(final_mnb.predict_proba(X_test_tvec), columns=['prob_socsci', 'prob_sci'])\n",
    "model_check = model_check.join(model_preds)\n",
    "model_check = model_check.join(y_test.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>prob_socsci</th>\n",
       "      <th>prob_sci</th>\n",
       "      <th>index</th>\n",
       "      <th>is_askscience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.263944</td>\n",
       "      <td>0.736056</td>\n",
       "      <td>477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.175433</td>\n",
       "      <td>0.824567</td>\n",
       "      <td>728</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.973111</td>\n",
       "      <td>0.026889</td>\n",
       "      <td>1003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.949183</td>\n",
       "      <td>0.050817</td>\n",
       "      <td>1009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.186830</td>\n",
       "      <td>0.813170</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>0</td>\n",
       "      <td>0.703571</td>\n",
       "      <td>0.296429</td>\n",
       "      <td>1473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>0</td>\n",
       "      <td>0.754031</td>\n",
       "      <td>0.245969</td>\n",
       "      <td>1634</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>1</td>\n",
       "      <td>0.036928</td>\n",
       "      <td>0.963072</td>\n",
       "      <td>549</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1</td>\n",
       "      <td>0.229482</td>\n",
       "      <td>0.770518</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>1</td>\n",
       "      <td>0.312388</td>\n",
       "      <td>0.687612</td>\n",
       "      <td>684</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pred  prob_socsci  prob_sci  index  is_askscience\n",
       "0       1     0.263944  0.736056    477              1\n",
       "1       1     0.175433  0.824567    728              1\n",
       "2       0     0.973111  0.026889   1003              0\n",
       "3       0     0.949183  0.050817   1009              0\n",
       "4       1     0.186830  0.813170    103              1\n",
       "..    ...          ...       ...    ...            ...\n",
       "544     0     0.703571  0.296429   1473              0\n",
       "545     0     0.754031  0.245969   1634              0\n",
       "546     1     0.036928  0.963072    549              1\n",
       "547     1     0.229482  0.770518    294              1\n",
       "548     1     0.312388  0.687612    684              1\n",
       "\n",
       "[549 rows x 5 columns]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>prob_socsci</th>\n",
       "      <th>prob_sci</th>\n",
       "      <th>index</th>\n",
       "      <th>is_askscience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0.658496</td>\n",
       "      <td>0.341504</td>\n",
       "      <td>852</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0</td>\n",
       "      <td>0.514766</td>\n",
       "      <td>0.485234</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1</td>\n",
       "      <td>0.351676</td>\n",
       "      <td>0.648324</td>\n",
       "      <td>1544</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0.513043</td>\n",
       "      <td>0.486957</td>\n",
       "      <td>458</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>0.785796</td>\n",
       "      <td>0.214204</td>\n",
       "      <td>634</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1</td>\n",
       "      <td>0.383292</td>\n",
       "      <td>0.616708</td>\n",
       "      <td>1095</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0</td>\n",
       "      <td>0.609846</td>\n",
       "      <td>0.390154</td>\n",
       "      <td>881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0</td>\n",
       "      <td>0.771974</td>\n",
       "      <td>0.228026</td>\n",
       "      <td>167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>0.432752</td>\n",
       "      <td>0.567248</td>\n",
       "      <td>1332</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1</td>\n",
       "      <td>0.494523</td>\n",
       "      <td>0.505477</td>\n",
       "      <td>1390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0</td>\n",
       "      <td>0.550860</td>\n",
       "      <td>0.449140</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0</td>\n",
       "      <td>0.733307</td>\n",
       "      <td>0.266693</td>\n",
       "      <td>547</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0</td>\n",
       "      <td>0.611932</td>\n",
       "      <td>0.388068</td>\n",
       "      <td>829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>1</td>\n",
       "      <td>0.235509</td>\n",
       "      <td>0.764491</td>\n",
       "      <td>1268</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0</td>\n",
       "      <td>0.536104</td>\n",
       "      <td>0.463896</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>1</td>\n",
       "      <td>0.401352</td>\n",
       "      <td>0.598648</td>\n",
       "      <td>1051</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0</td>\n",
       "      <td>0.542918</td>\n",
       "      <td>0.457082</td>\n",
       "      <td>606</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0</td>\n",
       "      <td>0.582612</td>\n",
       "      <td>0.417388</td>\n",
       "      <td>915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0</td>\n",
       "      <td>0.898604</td>\n",
       "      <td>0.101396</td>\n",
       "      <td>587</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0</td>\n",
       "      <td>0.535058</td>\n",
       "      <td>0.464942</td>\n",
       "      <td>938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>0</td>\n",
       "      <td>0.884259</td>\n",
       "      <td>0.115741</td>\n",
       "      <td>913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>0</td>\n",
       "      <td>0.887244</td>\n",
       "      <td>0.112756</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>0</td>\n",
       "      <td>0.532288</td>\n",
       "      <td>0.467712</td>\n",
       "      <td>710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pred  prob_socsci  prob_sci  index  is_askscience\n",
       "84      0     0.658496  0.341504    852              1\n",
       "117     0     0.514766  0.485234    131              1\n",
       "120     1     0.351676  0.648324   1544              0\n",
       "137     0     0.513043  0.486957    458              1\n",
       "144     0     0.785796  0.214204    634              1\n",
       "146     1     0.383292  0.616708   1095              0\n",
       "183     0     0.609846  0.390154    881              1\n",
       "192     0     0.771974  0.228026    167              1\n",
       "195     1     0.432752  0.567248   1332              0\n",
       "207     1     0.494523  0.505477   1390              0\n",
       "226     0     0.550860  0.449140     85              1\n",
       "263     0     0.733307  0.266693    547              1\n",
       "274     0     0.611932  0.388068    829              1\n",
       "322     1     0.235509  0.764491   1268              0\n",
       "357     0     0.536104  0.463896    125              1\n",
       "366     1     0.401352  0.598648   1051              0\n",
       "413     0     0.542918  0.457082    606              1\n",
       "421     0     0.582612  0.417388    915              1\n",
       "423     0     0.898604  0.101396    587              1\n",
       "430     0     0.535058  0.464942    938              1\n",
       "452     0     0.884259  0.115741    913              1\n",
       "462     0     0.887244  0.112756      0              1\n",
       "532     0     0.532288  0.467712    710              1"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Misclassified posts\n",
    "model_check[model_check['pred'] != model_check['is_askscience']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close Calls\n",
    "\n",
    "As can be seen, many of the misclassified posts are 'close calls', i.e. the model assigned probabilities for each subreddit that were close to each other. Let us examine one such case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('../data/combined_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is the Cardia (oesophagus-stomach opening) named so?\n",
      " I'm curious about the linguistics (?) and the reasoning behind whoever named that region, considering that everything heart-related is \"cardiac,\" but just recently I learned that anything related to the Cardia is also \"cardiac\".\n",
      "\n",
      "They both seem to be from the Greek word \"kardia\" (heart) according to Merriam Webster, so I'm curious if something got lost in translation or if the scientist naming that region just decided to be funny.\n"
     ]
    }
   ],
   "source": [
    "# False Negative: Predicted r/AskSocialScience, belongs to r/AskScience\n",
    "print(data_raw.iloc[458, [1,2]]['title'])\n",
    "print(data_raw.iloc[458, [1,2]]['selftext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good example of a post that would throw off our model. The post is discussing a scientific topic - biological organs (`cardiac`). However, the context of the question is that of social science - the linguistics behind the name of the biological organ. Posts like these could easily belong to either subreddit and fit in. Our model is unable to understand the intricacies and semantics that clue us as to where the post actually belongs, it is only able to pick up on the key words or tokens that signal to the model how it should be classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completely Wrong\n",
    "\n",
    "On the other hand, there are some posts with which our model was way off in predicting the classification probability. Let us examine such a post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is climate change boosting development of mountainous regions and therefor of more mountainous countries?\n",
      "Looking at constant decrease of snow got me wondering: Is proportion of tourists in areas where there's substantial amount of snow cover (mountainous and northern regions) increasing as a result? I for one would've loved to see Paris during winter, but seeing that there's no snow made snowy areas more lucrative for me as I could visit Paris at any time of year and the experience wouldn't be very different.\n",
      "\n",
      "How do you think will that develop mountainous rural areas and countries which are covered with those areas to substantial amount, like Switzerland or Austria?\n"
     ]
    }
   ],
   "source": [
    "# False Positive: Predicted r/AskScience, belongs to r/AskSocialScience\n",
    "print(data_raw.iloc[1268, [1,2]]['title'])\n",
    "print(data_raw.iloc[1268, [1,2]]['selftext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another good example of a post that would throw off our model - the post is clearly a question on social impact (development of mountainous regions). However, the premise of the question has to do with a concept that strongly signals to `r/AskScience` - climate change. There are more words relating to climate change than there are relating to development, even though the focus of the question is on development. Our model is unable to understand the semantics behind the question as a whole and thus classifies it wrongly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Improvements\n",
    "\n",
    "The first and foremost improvement would be, of course, to gather more data. Reddit API's limitations of 1000 posts constrains us to a limited dataset. Furthermore, the timespan of posts is also constrained and the data may thus be affected by certain events that do not consistently happen. For instance, our model showed us that the main features that predicted `r/AskScience` were related to Covid 19. However, this was definitely not the case before the year 2020, and perhaps our model would perform very differently if given the subreddit data for a much longer time frame.\n",
    "\n",
    "We could also try and incorporate more semantic concepts into our model such as a measure of sentiment analysis to help add another feature to classify posts. As shown above, solely relying on a bag-of-words model leads to some weaknesses in the model when it cannot detect underlying implications or semantics of a post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions & Recommendations\n",
    "\n",
    "Going back to our problem statement in the beginning, we have successfully created an effective classification model using NLP techniques. Our final model incorporating the `Tfidf Vectorizer` and the `Multinomial Naive Bayes Classifier` delivered great results in terms of classifying posts correctly, indicating high predictive performance. The model was also good for inference, as we were able to extract the feature importances from the model.\n",
    "\n",
    "While the actual predictions may not provide much value to Reddit moderators (since the posts by default have to belong to a subreddit), the classification probabilities and feature importances can be helpful for moderators of these subreddits to look at the underlying characteristics of posts. They can use these inferences to shape their moderation policies and influence the direction of the subreddit. For example, the `r/AskScience` moderators could discover that Covid-19 posts are becoming too prevalent in the subreddit and may want to limit such posts so that the other topics are not drowned out, thus preserving their identity as a subreddit for all forms of science, not just for viruses and immunology.\n",
    "\n",
    "Another potential use for our model would be to help moderators understand the underlying themes and characteristics of the members in the subreddit. Although it may not be as useful for this project, since the identities of the subreddits (`r/AskScience` & `r/AskSocialScience`) are quite clear for all, the same NLP processes could be applied to other subreddits which may not have such a distinct identity. Moderators could discover frequent and/or important terms from the model that help them understand the members' interests, then organise events like AMA's to boost their engagement with the community."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
